{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari] stable-baselines3[extra] torch tensorboard ale-py\n",
        "!pip install jupyter matplotlib\n"
      ],
      "metadata": {
        "id": "z5fW3XGL0SS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import numpy\n",
        "# Create Breakout environment with frame skip & preprocessing\n",
        "from gymnasium.wrappers import FrameStackObservation,AtariPreprocessing\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class TrackProgressCallback(BaseCallback):\n",
        "    def __init__(self, save_freq=100_000, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "        self.avg_rewards = []\n",
        "        self.save_freq = save_freq\n",
        "\n",
        "    def _on_step(self):\n",
        "        # Track episode reward\n",
        "        if len(self.locals[\"infos\"]) > 0 and \"episode\" in self.locals[\"infos\"][0]:\n",
        "            r = self.locals[\"infos\"][0][\"episode\"][\"r\"]\n",
        "            self.episode_rewards.append(r)\n",
        "            # Running mean for last 100 episodes\n",
        "            avg = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) > 0 else r\n",
        "            self.avg_rewards.append(avg)\n",
        "        # Save weights at intervals\n",
        "        if self.num_timesteps % self.save_freq == 0:\n",
        "            self.model.save(f\"dqn_breakout_step_{self.num_timesteps}\")\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self):\n",
        "        np.save(\"episode_rewards.npy\", np.array(self.episode_rewards))\n",
        "        np.save(\"average_rewards.npy\", np.array(self.avg_rewards))\n",
        "\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\",frameskip=1)\n",
        "env = AtariPreprocessing(env, terminal_on_life_loss=True)\n",
        "env = FrameStackObservation(env, stack_size=4)\n"
      ],
      "metadata": {
        "id": "oJMRw8d-0Ui8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.sb2_compat.rmsprop_tf_like import RMSpropTFLike\n",
        "\n",
        "model = A2C(\n",
        "    \"CnnPolicy\",\n",
        "    env,\n",
        "    learning_rate=7e-4,\n",
        "    n_steps=5,\n",
        "    gamma=0.99,\n",
        "    ent_coef=0.01,\n",
        "    vf_coef=0.5,\n",
        "    max_grad_norm=0.5,\n",
        "    rms_prop_eps=1e-5,\n",
        "    use_rms_prop=True,\n",
        "    policy_kwargs=dict(\n",
        "        optimizer_class=RMSpropTFLike,\n",
        "        optimizer_kwargs=dict(eps=1e-5)\n",
        "    ),\n",
        "    tensorboard_log=\"./tensorboard_log_a2c/\",\n",
        "    verbose=1,\n",
        "    device=\"cuda\"                      # Use GPU if available\n",
        ")\n",
        "\n",
        "progress = TrackProgressCallback(save_freq=100_000)\n",
        "\n",
        "model.learn(total_timesteps=3_000_000)\n",
        "\n",
        "\n",
        "model.save(\"a2c_breakout\")\n",
        "print(\"Finished training and tracking progress.\")"
      ],
      "metadata": {
        "id": "aL__v96_0Wfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "ep_rewards = np.load(\"episode_rewards.npy\")\n",
        "avg_rewards = np.load(\"average_rewards.npy\")\n",
        "\n",
        "plt.plot(avg_rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"100-episode Average Reward\")\n",
        "plt.title(\"Breakout DQN Learning Curve\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluation\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\",frameskip=1)\n",
        "env = AtariPreprocessing(env, terminal_on_life_loss=True)\n",
        "env = FrameStackObservation(env, stack_size=4)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "done, truncated = False, False\n",
        "total_reward = 0\n",
        "while not (done or truncated):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "print(\"Eval total reward:\", total_reward)"
      ],
      "metadata": {
        "id": "L3LDc1w20eJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, _ = env.reset()\n",
        "done, truncated = False, False\n",
        "total_reward = 0\n",
        "\n",
        "while not (done or truncated):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "print(\"Total reward from PPO agent:\", total_reward)\n"
      ],
      "metadata": {
        "id": "SsEchRDu0abk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")])\n",
        "eval_env = VecVideoRecorder(\n",
        "    eval_env,\n",
        "    \"./videos_ppo/\",\n",
        "    record_video_trigger=lambda step: True,\n",
        "    video_length=1000,\n",
        "    name_prefix=\"ppo-breakout\"\n",
        ")\n",
        "\n",
        "obs = eval_env.reset()\n",
        "for _ in range(1000):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, infos = eval_env.step(action)\n",
        "eval_env.close()\n"
      ],
      "metadata": {
        "id": "-aDvxqha0gLc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}