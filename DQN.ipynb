{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari] stable-baselines3[extra] torch tensorboard ale-py\n",
        "!pip install jupyter matplotlib\n"
      ],
      "metadata": {
        "id": "z5fW3XGL0SS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "import ale_py\n",
        "import numpy as np\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class TrackProgressCallback(BaseCallback):\n",
        "    def __init__(self, save_freq=100_000, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "        self.avg_rewards = []\n",
        "        self.save_freq = save_freq\n",
        "\n",
        "    def _on_step(self):\n",
        "        # Track episode reward\n",
        "        if len(self.locals[\"infos\"]) > 0 and \"episode\" in self.locals[\"infos\"][0]:\n",
        "            r = self.locals[\"infos\"][0][\"episode\"][\"r\"]\n",
        "            self.episode_rewards.append(r)\n",
        "            # Running mean for last 100 episodes\n",
        "            avg = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) > 0 else r\n",
        "            self.avg_rewards.append(avg)\n",
        "        # Save weights at intervals\n",
        "        if self.num_timesteps % self.save_freq == 0:\n",
        "            self.model.save(f\"dqn_breakout_step_{self.num_timesteps}\")\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self):\n",
        "        np.save(\"episode_rewards.npy\", np.array(self.episode_rewards))\n",
        "        np.save(\"average_rewards.npy\", np.array(self.avg_rewards))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJMRw8d-0Ui8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import FrameStackObservation,AtariPreprocessing\n",
        "from typing import Callable\n",
        "\n",
        "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
        "    \"\"\"\n",
        "    Linear learning rate schedule.\n",
        "\n",
        "    :param initial_value: Initial learning rate.\n",
        "    :return: schedule that computes\n",
        "      current learning rate depending on remaining progress\n",
        "    \"\"\"\n",
        "    def func(progress_remaining: float) -> float:\n",
        "        \"\"\"\n",
        "        Progress will decrease from 1 (beginning) to 0.\n",
        "\n",
        "        :param progress_remaining:\n",
        "        :return: current learning rate\n",
        "        \"\"\"\n",
        "        return progress_remaining * initial_value\n",
        "\n",
        "    return func\n",
        "\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\", frameskip=1)\n",
        "env = AtariPreprocessing(env, terminal_on_life_loss=True)\n",
        "env = FrameStackObservation(env, stack_size=4)\n",
        "\n",
        "model = DQN(\n",
        "    \"CnnPolicy\",\n",
        "    env,\n",
        "    verbose=1,\n",
        "    buffer_size=100_000,\n",
        "    learning_starts=10_000,\n",
        "    batch_size=32,\n",
        "    gamma=0.99,\n",
        "    train_freq=4,\n",
        "    learning_rate=linear_schedule(0.0001),\n",
        "    target_update_interval=10_000,\n",
        "    exploration_fraction=0.1,\n",
        "    exploration_final_eps=0.01,\n",
        "    tensorboard_log=\"./tensorboard_log/\",\n",
        "    device=\"cuda\"  # or \"cpu\" if no GPU\n",
        ")\n",
        "\n",
        "\n",
        "progress = TrackProgressCallback(save_freq=100_000)\n",
        "model.learn(total_timesteps=3_000_000,callback=progress)\n",
        "\n",
        "\n",
        "model.save(\"dqn_breakout_test\")\n",
        "model.save_replay_buffer(\"dqn_replay_buffer\")\n",
        "print(\"Test training completed.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "aL__v96_0Wfq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "ep_rewards = np.load(\"episode_rewards.npy\")\n",
        "avg_rewards = np.load(\"average_rewards.npy\")\n",
        "\n",
        "plt.plot(avg_rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"100-episode Average Reward\")\n",
        "plt.title(\"Breakout DQN Learning Curve\")\n",
        "plt.show()\n",
        "\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\",frameskip=1)\n",
        "env = AtariPreprocessing(env, terminal_on_life_loss=True)\n",
        "env = FrameStackObservation(env, stack_size=4)\n",
        "obs, _ = env.reset()\n",
        "done, truncated = False, False\n",
        "total_reward = 0\n",
        "\n",
        "while not (done or truncated):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "\n",
        "print(\"Total reward:\", total_reward)\n"
      ],
      "metadata": {
        "id": "L3LDc1w20eJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def make_env():\n",
        "    env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\",frameskip=1)\n",
        "    env = AtariPreprocessing(env, terminal_on_life_loss=True)\n",
        "    env = FrameStackObservation(env, stack_size=4)\n",
        "    return env\n",
        "\n",
        "env = DummyVecEnv([make_env])\n",
        "env = VecVideoRecorder(\n",
        "    env,\n",
        "    \"./videos/\",\n",
        "    record_video_trigger=lambda step: True,\n",
        "    video_length=1000,\n",
        "    name_prefix=\"dqn-breakout\"\n",
        ")\n",
        "\n",
        "obs = env.reset()\n",
        "for _ in range(1000):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, infos = env.step(action)\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "-aDvxqha0gLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "Video('/content/videos/dqn-breakout-step-0-to-step-1000.mp4', embed=True)\n"
      ],
      "metadata": {
        "id": "UP2Zx2NIa_pr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}